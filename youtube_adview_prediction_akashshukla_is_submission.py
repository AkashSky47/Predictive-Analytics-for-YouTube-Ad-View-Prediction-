# -*- coding: utf-8 -*-
"""Youtube_adview_Prediction_AkashShukla_IS_submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aUUqy6BtHM7ccr00RgypPynvtymmiAlj
"""

# Importing necessary libraries and datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import joblib
import re

# Loading the training data
train_data = pd.read_csv('train_lyst1720633807653.csv')

# Checking shape and datatype
print("Shape of the dataset:", train_data.shape)
print("Data types in the dataset:\n", train_data.dtypes)
print(train_data.head())

# Cleaning the dataset
# Converting columns to numeric
train_data['views'] = pd.to_numeric(train_data['views'], errors='coerce')
train_data['likes'] = pd.to_numeric(train_data['likes'], errors='coerce')
train_data['dislikes'] = pd.to_numeric(train_data['dislikes'], errors='coerce')
train_data['comment'] = pd.to_numeric(train_data['comment'], errors='coerce')

# Converting 'duration' to total seconds
def convert_duration_to_seconds(duration):
    match = re.match(r'PT(\d+H)?(\d+M)?(\d+S)?', duration)
    hours = int(match.group(1)[:-1]) if match.group(1) else 0
    minutes = int(match.group(2)[:-1]) if match.group(2) else 0
    seconds = int(match.group(3)[:-1]) if match.group(3) else 0
    return hours * 3600 + minutes * 60 + seconds

train_data['duration'] = train_data['duration'].apply(convert_duration_to_seconds)

# Dropping non-numeric columns and rows with NaN values
train_data_numeric = train_data.drop(['vidid', 'published', 'category'], axis=1).dropna()

# Visualizing the dataset
plt.figure(figsize=(10, 6))
sns.heatmap(train_data_numeric.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

train_data_numeric.hist(bins=30, figsize=(15, 10))
plt.show()

# Normalizing data and splitting into training and test sets
X = train_data_numeric.drop('adview', axis=1)
y = train_data_numeric['adview']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Training various models
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_pred_linear = linear_model.predict(X_test)
print(f'Linear Regression RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_linear))}')

svr_model = SVR(kernel='rbf')
svr_model.fit(X_train, y_train)
y_pred_svr = svr_model.predict(X_test)
print(f'Support Vector Regressor RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_svr))}')

tree_model = DecisionTreeRegressor()
tree_model.fit(X_train, y_train)
y_pred_tree = tree_model.predict(X_test)
print(f'Decision Tree Regressor RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_tree))}')

forest_model = RandomForestRegressor(n_estimators=100)
forest_model.fit(X_train, y_train)
y_pred_forest = forest_model.predict(X_test)
print(f'Random Forest Regressor RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_forest))}')

# Building and training an artificial neural network
ann_model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)
])

ann_model.compile(optimizer='adam', loss='mean_squared_error')
ann_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)

y_pred_ann = ann_model.predict(X_test)
print(f'ANN RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ann))}')

# Saving and loading the best model
best_model = ann_model  # Replacing with the model with the lowest RMSE
joblib.dump(best_model, 'best_model.pkl')

# Loading the test data and preprocessing
test_data = pd.read_csv('test_lyst1720633807653.csv')

test_data['views'] = pd.to_numeric(test_data['views'], errors='coerce')
test_data['likes'] = pd.to_numeric(test_data['likes'], errors='coerce')
test_data['dislikes'] = pd.to_numeric(test_data['dislikes'], errors='coerce')
test_data['comment'] = pd.to_numeric(test_data['comment'], errors='coerce')

test_data['duration'] = test_data['duration'].apply(convert_duration_to_seconds)

test_data_numeric = test_data.drop(['vidid', 'published', 'category'], axis=1).dropna()

X_test_data_scaled = scaler.transform(test_data_numeric)

loaded_model = joblib.load('best_model.pkl')
predictions = loaded_model.predict(X_test_data_scaled)

# Saving predictions
pd.DataFrame(predictions, columns=['adview']).to_csv('PredictedAdview.csv', index=False)

# Checking shapes of datasets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)
print("Test data shape:", X_test_data_scaled.shape)

print("Script completed successfully.")

print("Script completed successfully.")